{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract text from Pdf documents and apply NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Variable: UC_CODE_DIR has been set to default: /home/alexv84/Documents/GitHub/pdf_extract/src\n",
      "Environment Variable: UC_DATA_DIR has been set to default: /home/alexv84/Documents/Arbeit/Allianz/AZVers/data\n",
      "Environment Variable: UC_LANG_ID has been set to default: /home/alexv84/Documents/Arbeit/Allianz/AZVers/fasttext_langdetect\n",
      "Environment Variable: UC_DATA_PKG_DIR has been set to default: \n",
      "Environment Variable: UC_DB_CONNECTION has been set to default: postgresql://postgres...\n",
      "Environment Variable: UC_PORT has been set to default: 5000\n",
      "Environment Variable: UC_APP_CONNECTION has been set to default: 127.0.0.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'pdf_extract.services.file' from '/home/alexv84/Documents/GitHub/pdf_extract/src/pdf_extract/services/file.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pdfplumber, os\n",
    "import os.path as osp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from pdf_extract.services import file\n",
    "from pdf_extract.config import global_config as glob\n",
    "from pdf_extract.config import config\n",
    "from pdf_extract.utils import utils\n",
    "from importlib import reload\n",
    "import fasttext\n",
    "\n",
    "reload(glob)\n",
    "reload(config)\n",
    "reload(utils)\n",
    "reload(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "list_of_NEG_docs = os.listdir(osp.join(glob.UC_DATA_DIR, 'example_cvs', 'negatives'))\n",
    "n_neg = len(list_of_NEG_docs)\n",
    "\n",
    "list_of_POS_docs = os.listdir(osp.join(glob.UC_DATA_DIR, 'example_cvs', 'positives'))\n",
    "n_pos = len(list_of_POS_docs)\n",
    "\n",
    "print(n_pos)\n",
    "print(n_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in all Pdfs in directory:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Postives only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_corpus1 = pd.DataFrame(columns=['text', 'fname'])\n",
    "\n",
    "for z, fname in enumerate(list_of_POS_docs):\n",
    "    i, page_objects, text = 0, {}, \"\"\n",
    "    # The open method returns an instance of the pdfplumber.PDF class.\n",
    "    with pdfplumber.open(osp.join(glob.UC_DATA_DIR, \"example_cvs\",f\"positives/{fname}\")) as pdf:\n",
    "        while i < len(pdf.pages):\n",
    "            page = pdf.pages[i]\n",
    "            #print(pdf.metadata)\n",
    "            page_objects[str(i+1)] = page.extract_text(x_tolerance=1, y_tolerance=3) #.split('\\n')\n",
    "            text += page_objects[str(i+1)]\n",
    "            #print(f\"Page {i}\")\n",
    "            #print(page.extract_text())\n",
    "            i += 1\n",
    "    \n",
    "    raw_corpus1.loc[z] = [text,fname]  \n",
    "raw_corpus1['label'] = 'positive'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negatives only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_corpus2 = pd.DataFrame(columns=['text', 'fname'])\n",
    "\n",
    "for z, fname in enumerate(list_of_NEG_docs):\n",
    "    i, page_objects, text = 0, {}, \"\"\n",
    "    # The open method returns an instance of the pdfplumber.PDF class.\n",
    "    with pdfplumber.open(osp.join(glob.UC_DATA_DIR, \"example_cvs\",f\"negatives/{fname}\")) as pdf:\n",
    "        while i < len(pdf.pages):\n",
    "            page = pdf.pages[i]\n",
    "            page_objects[str(i+1)] = page.extract_text(x_tolerance=1, y_tolerance=3) #.split('\\n')\n",
    "            text += page_objects[str(i+1)]\n",
    "            i += 1\n",
    "    \n",
    "    raw_corpus2.loc[z] = [text,fname]   \n",
    "raw_corpus2['label'] = 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_corpus = pd.concat([raw_corpus1,raw_corpus2], axis=0, ignore_index=True)\n",
    "#raw_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#page_objects\n",
    "#page.extract_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#page_instance = page_objects['1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pdf.metadata\n",
    "#page_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19, 3)\n",
      "(9, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "reload(utils)\n",
    "\n",
    "#X_train, X_valid, X_test, y_train, y_valid, y_test = utils.train_test_split_extend(X['text_cleaned'], y=X['label'], test_size=[0.2, 0.2], random_state=42, shuffle=True)\n",
    "X_train, X_test, y_train, y_test = utils.train_test_split_extend(raw_corpus, y=raw_corpus['label'], test_size=[0.3], random_state=42, shuffle=True)\n",
    "\n",
    "print(X_train.shape)\n",
    "#print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "#print(X_valid.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/alexv84/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/alexv84/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/alexv84/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/alexv84/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using english language.\n",
      "Using 179 stop words.\n",
      "Added 8 stopword(s).\n",
      "Added 13 stopword(s).\n",
      "Adding custom stop words...\n",
      "Loading nltk stemmer.\n",
      "Setting to lower cases.\n",
      "Removing whitespaces.\n",
      "Applying word tokenizer.\n",
      "Removing custom stopwords.\n",
      "Removing punctuations.\n",
      "Removing numbers.\n",
      "Removing digits.\n",
      "Removing non-alphabetic characters.\n",
      "Removing short tokens.\n",
      "Applying stemming.\n",
      "Finished preprocessing!\n",
      "Setting to lower cases.\n",
      "Removing whitespaces.\n",
      "Applying word tokenizer.\n",
      "Removing custom stopwords.\n",
      "Removing punctuations.\n",
      "Removing numbers.\n",
      "Removing digits.\n",
      "Removing non-alphabetic characters.\n",
      "Removing short tokens.\n",
      "Applying stemming.\n",
      "Finished preprocessing!\n",
      "Setting to lower cases.\n",
      "Removing whitespaces.\n",
      "Applying word tokenizer.\n",
      "Removing custom stopwords.\n",
      "Removing punctuations.\n",
      "Removing numbers.\n",
      "Removing digits.\n",
      "Removing non-alphabetic characters.\n",
      "Removing short tokens.\n",
      "Applying stemming.\n",
      "Finished preprocessing!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer #, TfidfTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from pdf_extract.resources import preprocessor as preproc\n",
    "\n",
    "reload(preproc)\n",
    "\n",
    "# Preprocess corpus:\n",
    "cleaner = preproc.clean_text(language='english', lemma = False, stem = True)\n",
    "\n",
    "# Full sample\n",
    "X_cleaned = cleaner.fit_transform(raw_corpus['text'])      \n",
    "\n",
    "# Train\n",
    "X_train_cleaned = cleaner.fit_transform(X_train['text'])\n",
    "\n",
    "# Test\n",
    "X_test_cleaned = cleaner.transform(X_test['text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 75 candidates, totalling 375 fits\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    'vectorizer__max_df': (0.5, 0.75, 1.0),\n",
    "     'vectorizer__max_features': (None, 500, 1000, 5000, 10000),\n",
    "   #   'vectorizer__analyzer' : (\"word\", \"char\", \"char_wb\"),\n",
    "    'vectorizer__ngram_range': ((1, 1), (1, 2), (1, 3), (1, 4), (2, 2))  \n",
    "    #'vectorizer__ngram_range': ((1,1), (1, 2), (1, 3), (2, 3), (3, 3), (3, 5))  \n",
    "}\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "   #('cleaner', preproc.clean_text(language='english', lemma = False, stem = False)),\n",
    "#    ('vectorizer', CountVectorizer(lowercase=True, \n",
    "#                        #token_pattern = '(?u)(?:(?!\\d)\\w)+\\\\w+', \n",
    "#                        #analyzer = 'char_wb', \n",
    "#                        tokenizer = None, stop_words = None)), \n",
    "    ('vectorizer', TfidfVectorizer(lowercase=True, norm=\"l2\",\n",
    "                       #token_pattern = '(?u)(?:(?!\\d)\\w)+\\\\w+', \n",
    "                       analyzer = 'word',   # char_wb \n",
    "                    #    ngram_range=(1,3),\n",
    "                       tokenizer = None, stop_words = None)), \n",
    "   ('model', MultinomialNB())\n",
    "])\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n",
    "\n",
    "gs = grid_search.fit(X_train_cleaned, y_train)\n",
    "\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "#print(best_parameters)\n",
    "\n",
    "# best_model refit:\n",
    "trained = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickle Service Read from file: /home/alexv84/Documents/Arbeit/Allianz/AZVers/data/trained_model.pkl\n"
     ]
    }
   ],
   "source": [
    "#-------------\n",
    "# Save model:\n",
    "#-------------\n",
    "file.PickleService(path=\"trained_model.pkl\", root_path=glob.UC_DATA_DIR, is_df=False).doWrite(X=trained)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00        10\n",
      "    positive       1.00      1.00      1.00         9\n",
      "\n",
      "    accuracy                           1.00        19\n",
      "   macro avg       1.00      1.00      1.00        19\n",
      "weighted avg       1.00      1.00      1.00        19\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict:\n",
    "y_pred_train = trained.predict(X_train_cleaned)\n",
    "\n",
    "#print(confusion_matrix(test_set['label'], y_pred))\n",
    "print(classification_report(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 2]\n",
      " [1 3]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.60      0.67         5\n",
      "    positive       0.60      0.75      0.67         4\n",
      "\n",
      "    accuracy                           0.67         9\n",
      "   macro avg       0.68      0.68      0.67         9\n",
      "weighted avg       0.68      0.67      0.67         9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict:\n",
    "y_pred_test = trained.predict(X_test_cleaned)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred_test))\n",
    "print(classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use full corpus and estimate F1 controlling for train/test uncertainty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.625      0.82857143 0.625      0.8        0.76190476]\n",
      "0.73 F1 with standard deviation 0.09\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(trained, X_cleaned, raw_corpus['label'], cv=5, scoring='f1_macro')\n",
    "\n",
    "print(scores)\n",
    "print(\"%0.2f F1 with standard deviation %0.2f\" % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test['Prob. positive class'] = trained.predict_proba(X_test_cleaned)[:,1]\n",
    "X_train['Prob. positive class'] = trained.predict_proba(X_train_cleaned)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train.sort_values(by = 'Prob. positive class', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19, 2325)\n",
      "2325\n"
     ]
    }
   ],
   "source": [
    "tfidf = pipeline.named_steps['vectorizer']\n",
    "\n",
    "X_doc_term = tfidf.fit_transform(X_train_cleaned).toarray()\n",
    "\n",
    "print(X_doc_term.shape)         #doc-terms\n",
    "print(len(tfidf.get_feature_names_out()))        # terms\n",
    "\n",
    "# for i in tfidf.get_feature_names_out():\n",
    "#      print(i)\n",
    "\n",
    "#tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individually explain predictions on test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "class_names = ['negative', 'positive']\n",
    "explainer = LimeTextExplainer(class_names = class_names)\n",
    "\n",
    "idx = 2\n",
    "exp = explainer.explain_instance(X_test_cleaned.iloc[idx], trained.predict_proba, num_features=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document id: 12\n",
      "Documnet name: cv21.pdf\n",
      "Probability(\"positive\") = 0.6535903138646869\n",
      "True class: positive\n"
     ]
    }
   ],
   "source": [
    "print('Document id: %d' % X_test_cleaned.index[idx])\n",
    "print(f\"Documnet name: {X_test['fname'].iloc[idx]}\")\n",
    "print('Probability(\"positive\") =', trained.predict_proba(X_test_cleaned.to_frame().iloc[idx])[:,1][0])\n",
    "print('True class: %s' % y_test.iloc[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test.sort_values(by = 'Prob. positive class', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted features for linear surrogate model, which approximates the behaviour of the orig. classifier in the vicinity of the test example.\n",
    "#exp.as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "#fig = exp.as_pyplot_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exp.show_in_notebook(text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exp.save_to_file('/tmp/oi.html')      # save the fully contained html page to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from lime import submodular_pick\n",
    "\n",
    "#sp_obj = submodular_pick.SubmodularPick(explainer, X_test_cleaned.iloc[idx], trained.predict_proba, sample_size=2, num_features=5,num_exps_desired=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Globally explain model predictions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_sc = StandardScaler().fit_transform(X_doc_term)  # use train set doc-term matrix from tf-idf as design matrix\n",
    "\n",
    "lr = LogisticRegression(max_iter=50, C=0.3).fit(X_sc, y_pred_train)\n",
    "\n",
    "#print(lr.coef_)     # feature importances by absolute value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take absolute values and sort in descending order:\n",
    "abs_lr_weights = pd.DataFrame(np.abs(lr.coef_)).T.set_index(tfidf.get_feature_names_out()).sort_values(by=0, ascending=False)\n",
    "#abs_lr_weights.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_weights = pd.DataFrame(lr.coef_).T.set_index(tfidf.get_feature_names_out()).sort_values(by=0, ascending=False)\n",
    "# lr_weights.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('env_pdf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d49ecafdcaaa7827583b5b21ff598afa0c2165a9797f363685e472c1c2a4bb3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
