{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract text from Pdf documents and apply NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'pdf_extract.config.config' from '/home/alexv84/Documents/GitHub/pdf_extract/src/pdf_extract/config/config.py'>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pdfplumber, os\n",
    "import os.path as osp\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from pdf_extract.services import file\n",
    "from pdf_extract.config import global_config as glob\n",
    "from pdf_extract.config import config\n",
    "from importlib import reload\n",
    "import fasttext\n",
    "\n",
    "reload(glob)\n",
    "reload(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "list_of_NEG_docs = os.listdir(osp.join(glob.UC_DATA_DIR, 'example_cvs', 'negatives'))\n",
    "n_neg = len(list_of_NEG_docs)\n",
    "\n",
    "list_of_POS_docs = os.listdir(osp.join(glob.UC_DATA_DIR, 'example_cvs', 'positives'))\n",
    "n_pos = len(list_of_POS_docs)\n",
    "\n",
    "print(n_pos)\n",
    "print(n_neg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fname = 'CV4'\n",
    "\n",
    "# i, page_objects, text = 0, {}, []\n",
    "# # The open method returns an instance of the pdfplumber.PDF class.\n",
    "# with pdfplumber.open(osp.join(glob.UC_DATA_DIR, f\"example_cvs/{fname}.pdf\")) as pdf:\n",
    "#     while i < len(pdf.pages):\n",
    "#         page = pdf.pages[i]\n",
    "#         page_objects[str(i+1)] = page.extract_text(x_tolerance=1).split('\\n')\n",
    "#         #mystring = page_objects[str(i+1)].replace('\\r', '').replace('\\n', '')\n",
    "#         text += page_objects[str(i+1)]\n",
    "#         print(f\"Page {i}\")\n",
    "#         #print(page.extract_text())\n",
    "#         i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#page_objects['1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in all Pdfs in directory:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Postives only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_corpus1 = pd.DataFrame(columns=['text', 'label'])\n",
    "\n",
    "for z, fname in enumerate(list_of_POS_docs):\n",
    "    #print(fname)\n",
    "    i, page_objects, text = 0, {}, \"\"\n",
    "    # The open method returns an instance of the pdfplumber.PDF class.\n",
    "    with pdfplumber.open(osp.join(glob.UC_DATA_DIR, \"example_cvs\",f\"positives/{fname}\")) as pdf:\n",
    "        while i < len(pdf.pages):\n",
    "            page = pdf.pages[i]\n",
    "            page_objects[str(i+1)] = page.extract_text(x_tolerance=1, y_tolerance=3) #.split('\\n')\n",
    "            text += page_objects[str(i+1)]\n",
    "            #print(f\"Page {i}\")\n",
    "            #print(page.extract_text())\n",
    "            i += 1\n",
    "    \n",
    "    raw_corpus1.loc[z] = text  \n",
    "raw_corpus1['label'] = 'positive'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negatives only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_corpus2 = pd.DataFrame(columns=['text','label'])\n",
    "\n",
    "for z, fname in enumerate(list_of_NEG_docs):\n",
    "    #print(fname)\n",
    "    i, page_objects, text = 0, {}, \"\"\n",
    "    # The open method returns an instance of the pdfplumber.PDF class.\n",
    "    with pdfplumber.open(osp.join(glob.UC_DATA_DIR, \"example_cvs\",f\"negatives/{fname}\")) as pdf:\n",
    "        while i < len(pdf.pages):\n",
    "            page = pdf.pages[i]\n",
    "            page_objects[str(i+1)] = page.extract_text(x_tolerance=1, y_tolerance=3) #.split('\\n')\n",
    "            text += page_objects[str(i+1)]\n",
    "            #print(f\"Page {i}\")\n",
    "            #print(page.extract_text())\n",
    "            i += 1\n",
    "    \n",
    "    raw_corpus2.loc[z] = text   \n",
    "raw_corpus2['label'] = 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_corpus = pd.concat([raw_corpus1,raw_corpus2], axis=0, ignore_index=True)\n",
    "#raw_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#page_objects\n",
    "\n",
    "#page.extract_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_instance = page_objects['1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pdf.metadata\n",
    "#page_instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess text corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using english language.\n",
      "Using 179 stop words.\n",
      "Added 0 stopword(s).\n",
      "Adding custom stop words...\n",
      "Setting to lower cases.\n",
      "Removing whitespaces.\n",
      "Applying word tokenizer.\n",
      "Removing custom stopwords.\n",
      "Removing punctuations.\n",
      "Removing numbers.\n",
      "Removing digits.\n",
      "Removing non-alphabetic characters.\n",
      "Removing short tokens.\n",
      "Finished preprocessing!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/alexv84/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/alexv84/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from pdf_extract.resources import preprocessor as preproc\n",
    "\n",
    "reload(preproc)\n",
    "\n",
    "# Preprocess corpus:\n",
    "cleaner = preproc.clean_text(language='english', lemma = False, stem = False)\n",
    "\n",
    "X = raw_corpus['text']\n",
    "\n",
    "X_cl = cleaner.fit_transform(X)\n",
    "\n",
    "#docs = X_cl.tolist()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fasttext_label</th>\n",
       "      <th>text_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>__label__positive</td>\n",
       "      <td>ghassen makhlouf student computer science link...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>__label__positive</td>\n",
       "      <td>tobias watzel zusammenfassung experte machine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>__label__positive</td>\n",
       "      <td>gabriel kordon kordon mangfallstraße rosenheim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>__label__positive</td>\n",
       "      <td>ceyda akbulut phone email akbulut gmail linked...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>__label__positive</td>\n",
       "      <td>boyao zhang statistician data scientist boyao ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      fasttext_label                                       text_cleaned\n",
       "0  __label__positive  ghassen makhlouf student computer science link...\n",
       "1  __label__positive  tobias watzel zusammenfassung experte machine ...\n",
       "2  __label__positive  gabriel kordon kordon mangfallstraße rosenheim...\n",
       "3  __label__positive  ceyda akbulut phone email akbulut gmail linked...\n",
       "4  __label__positive  boyao zhang statistician data scientist boyao ..."
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_fct = lambda x: '__label__{}'.format(x.label)\n",
    "\n",
    "raw_corpus['text_cleaned'] = X_cl\n",
    "raw_corpus['fasttext_label'] = raw_corpus.apply(combine_fct, axis = 1)\n",
    "\n",
    "X = raw_corpus[['fasttext_label', 'text_cleaned']].reset_index(drop=True)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TXT Service output to file: /home/alexv84/Documents/Arbeit/Allianz/AZVers/data/train_data_fasttext.txt\n"
     ]
    }
   ],
   "source": [
    "reload(file)\n",
    "\n",
    "config_output = config.io['output']\n",
    "Path(glob.UC_DATA_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "file.TXTService(verbose=True, root_path=glob.UC_DATA_DIR, **config_output['service']['TXTService']).doWrite(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Using Skip-gram --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  3095\n",
      "Number of labels: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100.0% words/sec/thread:  406609 lr:  0.000000 avg.loss:  0.693146 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "#-------------\n",
    "#reload(utils)\n",
    "\n",
    "mtype = 'skipgram'    #'cbow'  \n",
    "\n",
    "print(f\"-- Using {'Skip-gram' if mtype == 'skipgram' else 'CBOW'} --\")\n",
    "# Training word embeddings: \n",
    "model = fasttext.train_supervised(input = os.path.join(glob.UC_DATA_DIR,'train_data_fasttext.txt'), wordNgrams = 2, dim=100, epoch=60, lr=0.001, verbose=2)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#page_instance.pdf.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#page_instance.extract_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "NER = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use pretrained language identification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'__label__de'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fasttext\n",
    "\n",
    "model = fasttext.load_model(osp.join(glob.UC_LANG_ID, 'lid.176.bin'))\n",
    "\n",
    "label, score = model.predict(\"Hallo ich studiere Mathe in München\", k=2)  # top 2 matching languages\n",
    "label[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "#raw_text = X.loc[i]\n",
    "raw_text = X_cl.loc[i]\n",
    "#raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text=\"The Indian Space Research Organisation or is the national space agency of India, headquartered in Bengaluru. It operates under Department of Space which is directly overseen by the Prime Minister of India while Chairman of ISRO acts as executive of DOS as well.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = NER(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for word in text1.ents:\n",
    "#     print(word.text,word.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Companies, agencies, institutions, etc.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"ORG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Countries, cities, states'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"GPE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#displacy.render(text1,style=\"ent\",jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL=\"https://www.zeebiz.com/markets/currency/news-cryptocurrency-news-today-june-12-bitcoin-dogecoin-shiba-inu-and-other-top-coins-prices-and-all-latest-updates-158490\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_content = requests.get(URL).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html_content, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "body=soup.body.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "body= body.replace('\\n', ' ')\n",
    "#body= body.replace('t', ' ')\n",
    "#body= body.replace('r', ' ')\n",
    "#body= body.replace('xa0', ' ')\n",
    "#body=re.sub(r'[^ws]', '', body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "text3= NER(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('env_pdf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d49ecafdcaaa7827583b5b21ff598afa0c2165a9797f363685e472c1c2a4bb3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
