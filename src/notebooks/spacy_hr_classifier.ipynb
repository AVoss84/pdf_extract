{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract text from Pdf documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber, os\n",
    "import os.path as osp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from pdf_extract.services import file\n",
    "from pdf_extract.config import global_config as glob\n",
    "from pdf_extract.config import config\n",
    "from pdf_extract.utils import utils\n",
    "from importlib import reload\n",
    "import fasttext\n",
    "\n",
    "reload(glob)\n",
    "reload(config)\n",
    "reload(utils);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "list_of_NEG_docs = os.listdir(osp.join(glob.UC_DATA_DIR, 'example_cvs', 'negatives'))\n",
    "n_neg = len(list_of_NEG_docs)\n",
    "\n",
    "list_of_POS_docs = os.listdir(osp.join(glob.UC_DATA_DIR, 'example_cvs', 'positives'))\n",
    "n_pos = len(list_of_POS_docs)\n",
    "\n",
    "print(n_pos)\n",
    "print(n_neg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in all Pdfs in directory:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Postives only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_corpus1 = pd.DataFrame(columns=['text', 'fname'])\n",
    "\n",
    "for z, fname in enumerate(list_of_POS_docs):\n",
    "    i, page_objects, text = 0, {}, \"\"\n",
    "    # The open method returns an instance of the pdfplumber.PDF class.\n",
    "    with pdfplumber.open(osp.join(glob.UC_DATA_DIR, \"example_cvs\",f\"positives/{fname}\")) as pdf:\n",
    "        while i < len(pdf.pages):\n",
    "            page = pdf.pages[i]\n",
    "            #print(pdf.metadata)\n",
    "            page_objects[str(i+1)] = page.extract_text(x_tolerance=1, y_tolerance=3) #.split('\\n')\n",
    "            text += page_objects[str(i+1)]\n",
    "            #print(f\"Page {i}\")\n",
    "            #print(page.extract_text())\n",
    "            i += 1\n",
    "    \n",
    "    raw_corpus1.loc[z] = [text,fname]  \n",
    "raw_corpus1['label'] = 'positive'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negatives only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_corpus2 = pd.DataFrame(columns=['text', 'fname'])\n",
    "\n",
    "for z, fname in enumerate(list_of_NEG_docs):\n",
    "    i, page_objects, text = 0, {}, \"\"\n",
    "    # The open method returns an instance of the pdfplumber.PDF class.\n",
    "    with pdfplumber.open(osp.join(glob.UC_DATA_DIR, \"example_cvs\",f\"negatives/{fname}\")) as pdf:\n",
    "        while i < len(pdf.pages):\n",
    "            page = pdf.pages[i]\n",
    "            page_objects[str(i+1)] = page.extract_text(x_tolerance=1, y_tolerance=3) #.split('\\n')\n",
    "            text += page_objects[str(i+1)]\n",
    "            i += 1\n",
    "    \n",
    "    raw_corpus2.loc[z] = [text,fname]   \n",
    "raw_corpus2['label'] = 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_corpus = pd.concat([raw_corpus1,raw_corpus2], axis=0, ignore_index=True)\n",
    "#raw_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess text corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using english language.\n",
      "Using 179 stop words.\n",
      "Added 8 stopword(s).\n",
      "Added 13 stopword(s).\n",
      "Adding custom stop words...\n",
      "Setting to lower cases.\n",
      "Removing whitespaces.\n",
      "Applying word tokenizer.\n",
      "Removing custom stopwords.\n",
      "Removing punctuations.\n",
      "Removing numbers.\n",
      "Removing digits.\n",
      "Removing non-alphabetic characters.\n",
      "Removing short tokens.\n",
      "Finished preprocessing!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/alexv84/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/alexv84/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/alexv84/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/alexv84/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from pdf_extract.resources import preprocessor as preproc\n",
    "\n",
    "reload(preproc)\n",
    "\n",
    "# Preprocess corpus:\n",
    "cleaner = preproc.clean_text(language='english', lemma = False, stem = False)\n",
    "\n",
    "X_raw = raw_corpus['text'].copy()\n",
    "\n",
    "# Full sample\n",
    "X_cleaned = cleaner.fit_transform(X_raw)   \n",
    "\n",
    "#X = raw_corpus['text']\n",
    "\n",
    "#X_cl = cleaner.fit_transform(X_raw)\n",
    "\n",
    "docs = X_cleaned.tolist()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_fct = lambda x: (x.label == 'positive')*1\n",
    "\n",
    "raw_corpus['text_cleaned'] = X_cleaned\n",
    "raw_corpus['target'] = raw_corpus.apply(combine_fct, axis = 1)\n",
    "\n",
    "X = raw_corpus[['target', 'text_cleaned', 'text']].reset_index(drop=True)   # keep label in X for txt-file below, will not be used directly in ft\n",
    "X.target = X.target #.astype(str)\n",
    "#X.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19,)\n",
      "(9,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[\"text_cleaned\"].values, X[\"target\"].values, test_size=0.3)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained spaCy model loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 43.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/alexv84/Documents/Arbeit/Allianz/AZVers/data/train.spacy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 57.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/alexv84/Documents/Arbeit/Allianz/AZVers/data/valid.spacy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<spacy.tokens._serialize.DocBin at 0x7f4227a1bc70>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pdf_extract.utils import spacy_text_classifier as clf\n",
    "\n",
    "reload(clf)\n",
    "\n",
    "sclf = clf.SpacyClassifier()\n",
    "\n",
    "#sclf._fill_config_file(base_config_file_name = \"base_config_textclf.cfg\", config_file_name = \"config.cfg\")\n",
    "\n",
    "sclf.create_docs(X_train, y_train, target_file=\"train.spacy\")\n",
    "sclf.create_docs(X_test, y_test, target_file=\"valid.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import configparser\n",
    "\n",
    "# configParser = configparser.RawConfigParser()   \n",
    "# configFilePath = os.path.join(glob.UC_DATA_DIR, \"output\", \"model-last\", \"config.cfg\")\n",
    "# configParser.read(configFilePath)\n",
    "\n",
    "# configParser.read_file(open(configFilePath))\n",
    "# item1 = configParser.get('paths', 'train')\n",
    "# item2 = configParser.get('paths', 'vectors')\n",
    "# item2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using base_config.cfg as base template.\n",
      "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
      "/home/alexv84/Documents/Arbeit/Allianz/AZVers/data/config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n",
      "\n",
      "\u001b[38;5;4mℹ Saving to output directory:\n",
      "/home/alexv84/Documents/Arbeit/Allianz/AZVers/data/output\u001b[0m\n",
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'textcat']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
      "E    #       LOSS TOK2VEC  LOSS TEXTCAT  CATS_SCORE  SCORE \n",
      "---  ------  ------------  ------------  ----------  ------\n",
      "  0       0          0.00          0.25       25.00    0.25\n",
      " 10     200          2.71          9.52       86.15    0.86\n",
      " 21     400          0.00          0.00       86.15    0.86\n",
      " 31     600          0.00          0.00       86.15    0.86\n",
      " 42     800          0.00          0.00       86.15    0.86\n",
      " 52    1000          0.00          0.00       86.15    0.86\n",
      " 63    1200          0.00          0.00       86.15    0.86\n",
      " 73    1400          0.00          0.00       86.15    0.86\n",
      " 84    1600          0.00          0.00       86.15    0.86\n",
      " 94    1800          0.00          0.00       86.15    0.86\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "/home/alexv84/Documents/Arbeit/Allianz/AZVers/data/output/model-last\n",
      "\n",
      "Training done!\n"
     ]
    }
   ],
   "source": [
    "trained = sclf.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000000e+00, 2.27031673e-08],\n",
       "       [9.99999404e-01, 5.68274288e-07],\n",
       "       [4.11547810e-11, 1.00000000e+00],\n",
       "       [1.00000000e+00, 1.08874483e-08],\n",
       "       [3.50249309e-07, 9.99999642e-01],\n",
       "       [9.99999642e-01, 2.98929422e-07],\n",
       "       [9.99999642e-01, 3.67472808e-07],\n",
       "       [9.99991179e-01, 8.79136951e-06],\n",
       "       [9.99996901e-01, 3.07706227e-06]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sclf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 1, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '0', '1', '0', '1', '0', '0', '0', '0']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[str(i) for i in sclf.predict(X_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8888888888888888"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sclf.score(X = X_test, y = y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         9\n",
      "           1       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           1.00        19\n",
      "   macro avg       1.00      1.00      1.00        19\n",
      "weighted avg       1.00      1.00      1.00        19\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      1.00      0.92         6\n",
      "           1       1.00      0.67      0.80         3\n",
      "\n",
      "    accuracy                           0.89         9\n",
      "   macro avg       0.93      0.83      0.86         9\n",
      "weighted avg       0.90      0.89      0.88         9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_train, sclf.predict(X_train), target_names=[\"0\",\"1\"]))\n",
    "print(classification_report(y_test, sclf.predict(X_test), target_names=[\"0\",\"1\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec',\n",
       " 'tagger',\n",
       " 'morphologizer',\n",
       " 'parser',\n",
       " 'lemmatizer',\n",
       " 'attribute_ruler',\n",
       " 'ner']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.tokens import DocBin\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "\n",
    "nlp = spacy.load(\"de_core_news_lg\") \n",
    "#nlp = spacy.load(\"de_core_web_lg\")     \n",
    "#nlp = spacy.blank('en')              # blank model; to train from scratch on own corpus\n",
    "\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Set, List, Tuple\n",
    "from spacy.tokens import DocBin\n",
    "\n",
    "def create_docs(data: List[Tuple[str, str]], target_file: str, cats: Set[str]):\n",
    "    \n",
    "    docs = DocBin()\n",
    "    for doc, label in nlp.pipe(data, as_tuples=True):\n",
    "        for cat in cats:\n",
    "            doc.cats[cat] = 1 if cat == label else 0\n",
    "        docs.add(doc)\n",
    "    docs.to_disk(os.path.join(glob.UC_DATA_DIR, target_file))\n",
    "    print(os.path.join(glob.UC_DATA_DIR, target_file))\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19,)\n",
      "(9,)\n",
      "/home/alexv84/Documents/Arbeit/Allianz/AZVers/data/train.spacy\n",
      "/home/alexv84/Documents/Arbeit/Allianz/AZVers/data/valid.spacy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<spacy.tokens._serialize.DocBin at 0x7f84dc24adf0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[\"text_cleaned\"].values, X[\"target\"].values, test_size=0.3)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "cats = [\"0\",\"1\"]\n",
    "\n",
    "create_docs(list(zip(X_train, y_train)), \"train.spacy\", cats=cats)\n",
    "create_docs(list(zip(X_test, y_test)), \"valid.spacy\", cats=cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import subprocess\n",
    "\n",
    "# base_config_file_name = \"base_config_textclf.cfg\"\n",
    "# config_file_name = \"config.cfg\"\n",
    "\n",
    "# cmd_init = 'python -m spacy init fill-config {}/{} {}/{}'.format(glob.UC_DATA_DIR, base_config_file_name, glob.UC_DATA_DIR, config_file_name)\n",
    "\n",
    "# process = subprocess.Popen(cmd_init.split(), stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "# stdout_cmd, stderr_cmd = process.communicate()\n",
    "# print(stdout_cmd.decode(\"utf-8\"))     # convert bytes to string for nicer printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
      "/home/alexv84/Documents/Arbeit/Allianz/AZVers/data/config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy init fill-config /home/alexv84/Documents/Arbeit/Allianz/AZVers/data/base_config_textclf.cfg /home/alexv84/Documents/Arbeit/Allianz/AZVers/data/config.cfg   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Saving to output directory:\n",
      "/home/alexv84/Documents/Arbeit/Allianz/AZVers/data/output\u001b[0m\n",
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'textcat']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
      "E    #       LOSS TOK2VEC  LOSS TEXTCAT  CATS_SCORE  SCORE \n",
      "---  ------  ------------  ------------  ----------  ------\n",
      "  0       0          0.00          0.25       35.71    0.36\n",
      " 10     200         90.71         20.48       77.50    0.77\n",
      " 21     400         41.17         12.40       35.71    0.36\n",
      " 31     600        229.46          7.70       35.71    0.36\n",
      " 42     800        324.93         20.71       50.00    0.50\n",
      " 52    1000          0.00          0.00       50.00    0.50\n",
      " 63    1200          0.00          0.00       50.00    0.50\n",
      " 73    1400          0.00          0.00       50.00    0.50\n",
      " 84    1600          0.00          0.00       50.00    0.50\n",
      " 94    1800          0.00          0.00       50.00    0.50\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "/home/alexv84/Documents/Arbeit/Allianz/AZVers/data/output/model-last\n",
      "\n",
      "Training done!\n"
     ]
    }
   ],
   "source": [
    "from spacy.cli.train import train as train_model\n",
    "\n",
    "train_model(config_path = f\"{glob.UC_DATA_DIR}/config.cfg\",\n",
    "        output_path = f\"{glob.UC_DATA_DIR}/output\",\n",
    "        overrides={\"paths.train\": f\"{glob.UC_DATA_DIR}/train.spacy\",\n",
    "                \"paths.dev\": f\"{glob.UC_DATA_DIR}/valid.spacy\",\n",
    "    },\n",
    ")\n",
    "\n",
    "print(\"\\nTraining done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use best model fit:\n",
    "trained_nlp = spacy.load(f\"{glob.UC_DATA_DIR}/output/model-best\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 2\n",
    "\n",
    "text = X['text_cleaned'][id]\n",
    "#print(text)\n",
    "print(X['target'][id])\n",
    "\n",
    "# Perform the trained pipeline on this text\n",
    "doc = trained_nlp(text)\n",
    "\n",
    "scores = doc.cats['0'], doc.cats['1']\n",
    "#predicted_class = np.array(scores).argmax(axis=1)\n",
    "#predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = X['text_cleaned'].head(2).tolist()\n",
    "#query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'0': 2.180798944206863e-12, '1': 1.0},\n",
       " {'0': 0.9999996423721313, '1': 3.010082991750096e-07}]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#trained_nlp.tokenizer(text)\n",
    "\n",
    "scores = [trained_nlp(text).cats for text in query]\n",
    "scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.180799e-12</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.999996e-01</td>\n",
       "      <td>3.010083e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0             1\n",
       "0  2.180799e-12  1.000000e+00\n",
       "1  9.999996e-01  3.010083e-07"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_class = pd.DataFrame(scores).values.argmax(axis=1)\n",
    "predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_from_pretrained = nlp(text)\n",
    "\n",
    "print(\"entities:\", doc_from_pretrained.ents)\n",
    "print(\"sentences:\", list(doc_from_pretrained.sents))\n",
    "print(\"classification:\", doc_from_pretrained.cats)    # empty -> add trained_nlp pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp = spacy.load(\"de_core_news_lg\", exclude=[\"textcat\"])\n",
    "#nlp.add_pipe(\"textcat\", source = trained_nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Companies, agencies, institutions, etc.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"ORG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Countries, cities, states'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"GPE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#displacy.render(text1,style=\"ent\",jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL=\"https://www.zeebiz.com/markets/currency/news-cryptocurrency-news-today-june-12-bitcoin-dogecoin-shiba-inu-and-other-top-coins-prices-and-all-latest-updates-158490\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_content = requests.get(URL).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html_content, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "body=soup.body.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "body= body.replace('\\n', ' ')\n",
    "#body= body.replace('t', ' ')\n",
    "#body= body.replace('r', ' ')\n",
    "#body= body.replace('xa0', ' ')\n",
    "#body=re.sub(r'[^ws]', '', body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "text3= NER(body)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('env_pdf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "0d49ecafdcaaa7827583b5b21ff598afa0c2165a9797f363685e472c1c2a4bb3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
