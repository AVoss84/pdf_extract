{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract text from Pdf documents and apply NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber, os\n",
    "import os.path as osp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from pdf_extract.services import file\n",
    "from pdf_extract.config import global_config as glob\n",
    "from pdf_extract.config import config\n",
    "from pdf_extract.utils import utils\n",
    "from importlib import reload\n",
    "import fasttext\n",
    "\n",
    "reload(glob)\n",
    "reload(config)\n",
    "reload(utils);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "list_of_NEG_docs = os.listdir(osp.join(glob.UC_DATA_DIR, 'example_cvs', 'negatives'))\n",
    "n_neg = len(list_of_NEG_docs)\n",
    "\n",
    "list_of_POS_docs = os.listdir(osp.join(glob.UC_DATA_DIR, 'example_cvs', 'positives'))\n",
    "n_pos = len(list_of_POS_docs)\n",
    "\n",
    "print(n_pos)\n",
    "print(n_neg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in all Pdfs in directory:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Postives only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_corpus1 = pd.DataFrame(columns=['text', 'fname'])\n",
    "\n",
    "for z, fname in enumerate(list_of_POS_docs):\n",
    "    i, page_objects, text = 0, {}, \"\"\n",
    "    # The open method returns an instance of the pdfplumber.PDF class.\n",
    "    with pdfplumber.open(osp.join(glob.UC_DATA_DIR, \"example_cvs\",f\"positives/{fname}\")) as pdf:\n",
    "        while i < len(pdf.pages):\n",
    "            page = pdf.pages[i]\n",
    "            #print(pdf.metadata)\n",
    "            page_objects[str(i+1)] = page.extract_text(x_tolerance=1, y_tolerance=3) #.split('\\n')\n",
    "            text += page_objects[str(i+1)]\n",
    "            #print(f\"Page {i}\")\n",
    "            #print(page.extract_text())\n",
    "            i += 1\n",
    "    \n",
    "    raw_corpus1.loc[z] = [text,fname]  \n",
    "raw_corpus1['label'] = 'positive'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negatives only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_corpus2 = pd.DataFrame(columns=['text', 'fname'])\n",
    "\n",
    "for z, fname in enumerate(list_of_NEG_docs):\n",
    "    i, page_objects, text = 0, {}, \"\"\n",
    "    # The open method returns an instance of the pdfplumber.PDF class.\n",
    "    with pdfplumber.open(osp.join(glob.UC_DATA_DIR, \"example_cvs\",f\"negatives/{fname}\")) as pdf:\n",
    "        while i < len(pdf.pages):\n",
    "            page = pdf.pages[i]\n",
    "            page_objects[str(i+1)] = page.extract_text(x_tolerance=1, y_tolerance=3) #.split('\\n')\n",
    "            text += page_objects[str(i+1)]\n",
    "            i += 1\n",
    "    \n",
    "    raw_corpus2.loc[z] = [text,fname]   \n",
    "raw_corpus2['label'] = 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_corpus = pd.concat([raw_corpus1,raw_corpus2], axis=0, ignore_index=True)\n",
    "#raw_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess text corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using english language.\n",
      "Using 179 stop words.\n",
      "Added 8 stopword(s).\n",
      "Added 13 stopword(s).\n",
      "Adding custom stop words...\n",
      "Setting to lower cases.\n",
      "Removing whitespaces.\n",
      "Applying word tokenizer.\n",
      "Removing custom stopwords.\n",
      "Removing punctuations.\n",
      "Removing numbers.\n",
      "Removing digits.\n",
      "Removing non-alphabetic characters.\n",
      "Removing short tokens.\n",
      "Finished preprocessing!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/alexv84/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/alexv84/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/alexv84/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/alexv84/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from pdf_extract.resources import preprocessor as preproc\n",
    "\n",
    "reload(preproc)\n",
    "\n",
    "# Preprocess corpus:\n",
    "cleaner = preproc.clean_text(language='english', lemma = False, stem = False)\n",
    "\n",
    "X_raw = raw_corpus['text'].copy()\n",
    "\n",
    "# Full sample\n",
    "X_cleaned = cleaner.fit_transform(X_raw)   \n",
    "\n",
    "#X = raw_corpus['text']\n",
    "\n",
    "#X_cl = cleaner.fit_transform(X_raw)\n",
    "\n",
    "docs = X_cleaned.tolist()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_fct = lambda x: (x.label == 'positive')*1\n",
    "\n",
    "raw_corpus['text_cleaned'] = X_cleaned\n",
    "raw_corpus['target'] = raw_corpus.apply(combine_fct, axis = 1)\n",
    "\n",
    "X = raw_corpus[['target', 'text_cleaned', 'text']].reset_index(drop=True)   # keep label in X for txt-file below, will not be used directly in ft\n",
    "X.target = X.target.astype(str)\n",
    "#X.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19,)\n",
      "(9,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[\"text_cleaned\"].values, X[\"target\"].values, test_size=0.3)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained spaCy model loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 44.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/alexv84/Documents/Arbeit/Allianz/AZVers/data/train.spacy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 51.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/alexv84/Documents/Arbeit/Allianz/AZVers/data/valid.spacy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<spacy.tokens._serialize.DocBin at 0x7f41e6d18fd0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pdf_extract.utils import spacy_text_classifier as clf\n",
    "\n",
    "reload(clf)\n",
    "\n",
    "sclf = clf.spacy_classifier()\n",
    "\n",
    "#sclf._fill_config_file(base_config_file_name = \"base_config_textclf.cfg\", config_file_name = \"config.cfg\")\n",
    "\n",
    "sclf.create_docs(X_train, y_train, target_file=\"train.spacy\")\n",
    "sclf.create_docs(X_test, y_test, target_file=\"valid.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using base_config.cfg as base config file.\n",
      "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
      "/home/alexv84/Documents/Arbeit/Allianz/AZVers/data/config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n",
      "\n",
      "\u001b[38;5;4mℹ Saving to output directory:\n",
      "/home/alexv84/Documents/Arbeit/Allianz/AZVers/data/output\u001b[0m\n",
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'textcat']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
      "E    #       LOSS TOK2VEC  LOSS TEXTCAT  CATS_SCORE  SCORE \n",
      "---  ------  ------------  ------------  ----------  ------\n",
      "  0       0          0.00          0.25       40.00    0.40\n",
      " 10     200         44.43         28.96       41.56    0.42\n",
      " 21     400          8.72          3.65       41.56    0.42\n",
      " 31     600          0.00          0.00       41.56    0.42\n"
     ]
    }
   ],
   "source": [
    "trained = sclf.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.45956836e-08, 1.00000000e+00],\n",
       "       [1.00000000e+00, 1.58310574e-08],\n",
       "       [2.15811184e-07, 9.99999762e-01],\n",
       "       [3.04064951e-09, 1.00000000e+00],\n",
       "       [1.04055896e-01, 8.95944059e-01],\n",
       "       [9.99126256e-01, 8.73786397e-04],\n",
       "       [4.04459745e-01, 5.95540285e-01],\n",
       "       [4.39647152e-07, 9.99999523e-01],\n",
       "       [9.98532891e-01, 1.46706449e-03]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sclf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1', '0', '1', '1', '1', '0', '1', '1', '0'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '0', '1', '1', '1', '0', '1', '1', '0']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[str(i) for i in sclf.predict(X_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sclf.score(X = X_test, y = y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        12\n",
      "           1       1.00      1.00      1.00         7\n",
      "\n",
      "    accuracy                           1.00        19\n",
      "   macro avg       1.00      1.00      1.00        19\n",
      "weighted avg       1.00      1.00      1.00        19\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         3\n",
      "           1       1.00      1.00      1.00         6\n",
      "\n",
      "    accuracy                           1.00         9\n",
      "   macro avg       1.00      1.00      1.00         9\n",
      "weighted avg       1.00      1.00      1.00         9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_train, [str(i) for i in sclf.predict(X_train)], target_names=[\"0\",\"1\"]))\n",
    "print(classification_report(y_test, [str(i) for i in sclf.predict(X_test)], target_names=[\"0\",\"1\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec',\n",
       " 'tagger',\n",
       " 'morphologizer',\n",
       " 'parser',\n",
       " 'lemmatizer',\n",
       " 'attribute_ruler',\n",
       " 'ner']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.tokens import DocBin\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "\n",
    "nlp = spacy.load(\"de_core_news_lg\") \n",
    "#nlp = spacy.load(\"de_core_web_lg\")     \n",
    "#nlp = spacy.blank('en')              # blank model; to train from scratch on own corpus\n",
    "\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Set, List, Tuple\n",
    "from spacy.tokens import DocBin\n",
    "\n",
    "def create_docs(data: List[Tuple[str, str]], target_file: str, cats: Set[str]):\n",
    "    \n",
    "    docs = DocBin()\n",
    "    for doc, label in nlp.pipe(data, as_tuples=True):\n",
    "        for cat in cats:\n",
    "            doc.cats[cat] = 1 if cat == label else 0\n",
    "        docs.add(doc)\n",
    "    docs.to_disk(os.path.join(glob.UC_DATA_DIR, target_file))\n",
    "    print(os.path.join(glob.UC_DATA_DIR, target_file))\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19,)\n",
      "(9,)\n",
      "/home/alexv84/Documents/Arbeit/Allianz/AZVers/data/train.spacy\n",
      "/home/alexv84/Documents/Arbeit/Allianz/AZVers/data/valid.spacy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<spacy.tokens._serialize.DocBin at 0x7f84dc24adf0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[\"text_cleaned\"].values, X[\"target\"].values, test_size=0.3)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "cats = [\"0\",\"1\"]\n",
    "\n",
    "create_docs(list(zip(X_train, y_train)), \"train.spacy\", cats=cats)\n",
    "create_docs(list(zip(X_test, y_test)), \"valid.spacy\", cats=cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import subprocess\n",
    "\n",
    "# base_config_file_name = \"base_config_textclf.cfg\"\n",
    "# config_file_name = \"config.cfg\"\n",
    "\n",
    "# cmd_init = 'python -m spacy init fill-config {}/{} {}/{}'.format(glob.UC_DATA_DIR, base_config_file_name, glob.UC_DATA_DIR, config_file_name)\n",
    "\n",
    "# process = subprocess.Popen(cmd_init.split(), stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "# stdout_cmd, stderr_cmd = process.communicate()\n",
    "# print(stdout_cmd.decode(\"utf-8\"))     # convert bytes to string for nicer printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
      "/home/alexv84/Documents/Arbeit/Allianz/AZVers/data/config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy init fill-config /home/alexv84/Documents/Arbeit/Allianz/AZVers/data/base_config_textclf.cfg /home/alexv84/Documents/Arbeit/Allianz/AZVers/data/config.cfg   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Saving to output directory:\n",
      "/home/alexv84/Documents/Arbeit/Allianz/AZVers/data/output\u001b[0m\n",
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'textcat']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
      "E    #       LOSS TOK2VEC  LOSS TEXTCAT  CATS_SCORE  SCORE \n",
      "---  ------  ------------  ------------  ----------  ------\n",
      "  0       0          0.00          0.25       35.71    0.36\n",
      " 10     200         90.71         20.48       77.50    0.77\n",
      " 21     400         41.17         12.40       35.71    0.36\n",
      " 31     600        229.46          7.70       35.71    0.36\n",
      " 42     800        324.93         20.71       50.00    0.50\n",
      " 52    1000          0.00          0.00       50.00    0.50\n",
      " 63    1200          0.00          0.00       50.00    0.50\n",
      " 73    1400          0.00          0.00       50.00    0.50\n",
      " 84    1600          0.00          0.00       50.00    0.50\n",
      " 94    1800          0.00          0.00       50.00    0.50\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "/home/alexv84/Documents/Arbeit/Allianz/AZVers/data/output/model-last\n",
      "\n",
      "Training done!\n"
     ]
    }
   ],
   "source": [
    "from spacy.cli.train import train as train_model\n",
    "\n",
    "train_model(config_path = f\"{glob.UC_DATA_DIR}/config.cfg\",\n",
    "        output_path = f\"{glob.UC_DATA_DIR}/output\",\n",
    "        overrides={\"paths.train\": f\"{glob.UC_DATA_DIR}/train.spacy\",\n",
    "                \"paths.dev\": f\"{glob.UC_DATA_DIR}/valid.spacy\",\n",
    "    },\n",
    ")\n",
    "\n",
    "print(\"\\nTraining done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use best model fit:\n",
    "trained_nlp = spacy.load(f\"{glob.UC_DATA_DIR}/output/model-best\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gabriel kordon kordon mangfallstraße rosenheim persönliche daten name gabriel kordon geboren aibling aktueller wohnort mangfallstraße rosenheim kontaktinformation mobil mail kordon schul berufsausbildung ende technische universität münchen angestrebter abschluss mathematik master mixed poisson model lassa fever disease nigeria technische naturwissenschaftliche universität norwegens ntnu erasmus student abschluss mathematik bachelorarbeit regressionsanalyse ignaz günther gymnasium rosenheim abschluss allgemeine hochschulreife jobs praktika sept kpmg werkstudent bereich data science financial services cfro insurance projektarbeit einem großen deutschen versicherer münchener verein werkstudent bereich life insurance praktikum aschemann sternstunden catering service tender host expo real seit jahren rosenheim raubling tennistraining angehender trainer leistungssport schreinerei marschner einwöchiges praktikum ehrenamt seit jugendwart rosenheim besondere fähigkeiten fundierte kenntnisse computersprache insbesondere statistischen paketen modellbildung time series visualisierung ggplot programmierkenntnisse sprachen matlab julia ausgeprägte skills microsoft englisch kenntnisse niveau rosenheim\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9.196549077827854e-11, 1.0)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id = 2\n",
    "\n",
    "text = X['text_cleaned'][id]\n",
    "#print(text)\n",
    "print(X['target'][id])\n",
    "\n",
    "# Perform the trained pipeline on this text\n",
    "doc = trained_nlp(text)\n",
    "\n",
    "scores = doc.cats['0'], doc.cats['1']\n",
    "scores\n",
    "#predicted_class = np.array(scores).argmax(axis=1)\n",
    "#predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = X['text_cleaned'].head(2).tolist()\n",
    "#query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'0': 2.180798944206863e-12, '1': 1.0},\n",
       " {'0': 0.9999996423721313, '1': 3.010082991750096e-07}]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#trained_nlp.tokenizer(text)\n",
    "\n",
    "scores = [trained_nlp(text).cats for text in query]\n",
    "scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.180799e-12</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.999996e-01</td>\n",
       "      <td>3.010083e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0             1\n",
       "0  2.180799e-12  1.000000e+00\n",
       "1  9.999996e-01  3.010083e-07"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_class = pd.DataFrame(scores).values.argmax(axis=1)\n",
    "predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_from_pretrained = nlp(text)\n",
    "\n",
    "print(\"entities:\", doc_from_pretrained.ents)\n",
    "print(\"sentences:\", list(doc_from_pretrained.sents))\n",
    "print(\"classification:\", doc_from_pretrained.cats)    # empty -> add trained_nlp pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp = spacy.load(\"de_core_news_lg\", exclude=[\"textcat\"])\n",
    "#nlp.add_pipe(\"textcat\", source = trained_nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Companies, agencies, institutions, etc.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"ORG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Countries, cities, states'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"GPE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#displacy.render(text1,style=\"ent\",jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL=\"https://www.zeebiz.com/markets/currency/news-cryptocurrency-news-today-june-12-bitcoin-dogecoin-shiba-inu-and-other-top-coins-prices-and-all-latest-updates-158490\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_content = requests.get(URL).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html_content, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "body=soup.body.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "body= body.replace('\\n', ' ')\n",
    "#body= body.replace('t', ' ')\n",
    "#body= body.replace('r', ' ')\n",
    "#body= body.replace('xa0', ' ')\n",
    "#body=re.sub(r'[^ws]', '', body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "text3= NER(body)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('env_pdf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d49ecafdcaaa7827583b5b21ff598afa0c2165a9797f363685e472c1c2a4bb3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
