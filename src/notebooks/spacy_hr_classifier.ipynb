{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract text from Pdf documents and apply NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber, os\n",
    "import os.path as osp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from pdf_extract.services import file\n",
    "from pdf_extract.config import global_config as glob\n",
    "from pdf_extract.config import config\n",
    "from pdf_extract.utils import utils\n",
    "from importlib import reload\n",
    "import fasttext\n",
    "\n",
    "reload(glob)\n",
    "reload(config)\n",
    "reload(utils);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "list_of_NEG_docs = os.listdir(osp.join(glob.UC_DATA_DIR, 'example_cvs', 'negatives'))\n",
    "n_neg = len(list_of_NEG_docs)\n",
    "\n",
    "list_of_POS_docs = os.listdir(osp.join(glob.UC_DATA_DIR, 'example_cvs', 'positives'))\n",
    "n_pos = len(list_of_POS_docs)\n",
    "\n",
    "print(n_pos)\n",
    "print(n_neg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fname = 'CV4'\n",
    "\n",
    "# i, page_objects, text = 0, {}, []\n",
    "# # The open method returns an instance of the pdfplumber.PDF class.\n",
    "# with pdfplumber.open(osp.join(glob.UC_DATA_DIR, f\"example_cvs/{fname}.pdf\")) as pdf:\n",
    "#     while i < len(pdf.pages):\n",
    "#         page = pdf.pages[i]\n",
    "#         page_objects[str(i+1)] = page.extract_text(x_tolerance=1).split('\\n')\n",
    "#         #mystring = page_objects[str(i+1)].replace('\\r', '').replace('\\n', '')\n",
    "#         text += page_objects[str(i+1)]\n",
    "#         print(f\"Page {i}\")\n",
    "#         #print(page.extract_text())\n",
    "#         i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#page_objects['1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in all Pdfs in directory:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Postives only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_corpus1 = pd.DataFrame(columns=['text', 'fname'])\n",
    "\n",
    "for z, fname in enumerate(list_of_POS_docs):\n",
    "    i, page_objects, text = 0, {}, \"\"\n",
    "    # The open method returns an instance of the pdfplumber.PDF class.\n",
    "    with pdfplumber.open(osp.join(glob.UC_DATA_DIR, \"example_cvs\",f\"positives/{fname}\")) as pdf:\n",
    "        while i < len(pdf.pages):\n",
    "            page = pdf.pages[i]\n",
    "            #print(pdf.metadata)\n",
    "            page_objects[str(i+1)] = page.extract_text(x_tolerance=1, y_tolerance=3) #.split('\\n')\n",
    "            text += page_objects[str(i+1)]\n",
    "            #print(f\"Page {i}\")\n",
    "            #print(page.extract_text())\n",
    "            i += 1\n",
    "    \n",
    "    raw_corpus1.loc[z] = [text,fname]  \n",
    "raw_corpus1['label'] = 'positive'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negatives only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_corpus2 = pd.DataFrame(columns=['text', 'fname'])\n",
    "\n",
    "for z, fname in enumerate(list_of_NEG_docs):\n",
    "    i, page_objects, text = 0, {}, \"\"\n",
    "    # The open method returns an instance of the pdfplumber.PDF class.\n",
    "    with pdfplumber.open(osp.join(glob.UC_DATA_DIR, \"example_cvs\",f\"negatives/{fname}\")) as pdf:\n",
    "        while i < len(pdf.pages):\n",
    "            page = pdf.pages[i]\n",
    "            page_objects[str(i+1)] = page.extract_text(x_tolerance=1, y_tolerance=3) #.split('\\n')\n",
    "            text += page_objects[str(i+1)]\n",
    "            i += 1\n",
    "    \n",
    "    raw_corpus2.loc[z] = [text,fname]   \n",
    "raw_corpus2['label'] = 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_corpus = pd.concat([raw_corpus1,raw_corpus2], axis=0, ignore_index=True)\n",
    "#raw_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#page_objects\n",
    "\n",
    "#page.extract_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#page_instance = page_objects['1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pdf.metadata\n",
    "#page_instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess text corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using english language.\n",
      "Using 179 stop words.\n",
      "Added 8 stopword(s).\n",
      "Added 13 stopword(s).\n",
      "Adding custom stop words...\n",
      "Setting to lower cases.\n",
      "Removing whitespaces.\n",
      "Applying word tokenizer.\n",
      "Removing custom stopwords.\n",
      "Removing punctuations.\n",
      "Removing numbers.\n",
      "Removing digits.\n",
      "Removing non-alphabetic characters.\n",
      "Removing short tokens.\n",
      "Finished preprocessing!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/alexv84/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/alexv84/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from pdf_extract.resources import preprocessor as preproc\n",
    "\n",
    "reload(preproc)\n",
    "\n",
    "# Preprocess corpus:\n",
    "cleaner = preproc.clean_text(language='english', lemma = False, stem = False)\n",
    "\n",
    "X_raw = raw_corpus['text'].copy()\n",
    "\n",
    "# Full sample\n",
    "X_cleaned = cleaner.fit_transform(X_raw)   \n",
    "\n",
    "#X = raw_corpus['text']\n",
    "\n",
    "#X_cl = cleaner.fit_transform(X_raw)\n",
    "\n",
    "docs = X_cleaned.tolist()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>fname</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ghassen BEN MAKHLOUF\\nStudent | M.Sc in Comput...</td>\n",
       "      <td>cv17.pdf</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tobias Watzel\\nZUSAMMENFASSUNG\\nExperte für Ma...</td>\n",
       "      <td>cv20.pdf</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gabriel Kordon \\n \\n \\n+49 157 31683...</td>\n",
       "      <td>cv18.pdf</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CEYDA AKBULUT\\nM . S c .   D a t a   E n t h u...</td>\n",
       "      <td>cv15.pdf</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D R . A L E X A N D E R V O S S E L E R\\nperso...</td>\n",
       "      <td>cv26.pdf</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     fname     label\n",
       "0  Ghassen BEN MAKHLOUF\\nStudent | M.Sc in Comput...  cv17.pdf  positive\n",
       "1  Tobias Watzel\\nZUSAMMENFASSUNG\\nExperte für Ma...  cv20.pdf  positive\n",
       "2            Gabriel Kordon \\n \\n \\n+49 157 31683...  cv18.pdf  positive\n",
       "3  CEYDA AKBULUT\\nM . S c .   D a t a   E n t h u...  cv15.pdf  positive\n",
       "4  D R . A L E X A N D E R V O S S E L E R\\nperso...  cv26.pdf  positive"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text_cleaned</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>ghassen makhlouf student computer science link...</td>\n",
       "      <td>Ghassen BEN MAKHLOUF\\nStudent | M.Sc in Comput...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>tobias watzel zusammenfassung experte machine ...</td>\n",
       "      <td>Tobias Watzel\\nZUSAMMENFASSUNG\\nExperte für Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>gabriel kordon kordon mangfallstraße rosenheim...</td>\n",
       "      <td>Gabriel Kordon \\n \\n \\n+49 157 31683...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>ceyda akbulut phone email akbulut gmail linked...</td>\n",
       "      <td>CEYDA AKBULUT\\nM . S c .   D a t a   E n t h u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>personal information born sindelﬁngen germany ...</td>\n",
       "      <td>D R . A L E X A N D E R V O S S E L E R\\nperso...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target                                       text_cleaned  \\\n",
       "0      1  ghassen makhlouf student computer science link...   \n",
       "1      1  tobias watzel zusammenfassung experte machine ...   \n",
       "2      1  gabriel kordon kordon mangfallstraße rosenheim...   \n",
       "3      1  ceyda akbulut phone email akbulut gmail linked...   \n",
       "4      1  personal information born sindelﬁngen germany ...   \n",
       "\n",
       "                                                text  \n",
       "0  Ghassen BEN MAKHLOUF\\nStudent | M.Sc in Comput...  \n",
       "1  Tobias Watzel\\nZUSAMMENFASSUNG\\nExperte für Ma...  \n",
       "2            Gabriel Kordon \\n \\n \\n+49 157 31683...  \n",
       "3  CEYDA AKBULUT\\nM . S c .   D a t a   E n t h u...  \n",
       "4  D R . A L E X A N D E R V O S S E L E R\\nperso...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_fct = lambda x: (x.label == 'positive')*1\n",
    "\n",
    "raw_corpus['text_cleaned'] = X_cleaned\n",
    "raw_corpus['target'] = raw_corpus.apply(combine_fct, axis = 1)\n",
    "\n",
    "X = raw_corpus[['target', 'text_cleaned', 'text']].reset_index(drop=True)   # keep label in X for txt-file below, will not be used directly in ft\n",
    "X.target = X.target.astype(str)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec',\n",
       " 'tagger',\n",
       " 'morphologizer',\n",
       " 'parser',\n",
       " 'lemmatizer',\n",
       " 'attribute_ruler',\n",
       " 'ner']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.tokens import DocBin\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "\n",
    "nlp = spacy.load(\"de_core_news_lg\") \n",
    "#nlp = spacy.blank('en')              # blank model; to train from scratch on own corpus\n",
    "\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Set, List, Tuple\n",
    "from spacy.tokens import DocBin\n",
    "\n",
    "def make_docs(data: List[Tuple[str, str]], target_file: str, cats: Set[str]):\n",
    "    \n",
    "    docs = DocBin()\n",
    "    for doc, label in nlp.pipe(data, as_tuples=True):\n",
    "        for cat in cats:\n",
    "            doc.cats[cat] = 1 if cat == label else 0\n",
    "        docs.add(doc)\n",
    "    docs.to_disk(os.path.join(glob.UC_DATA_DIR, target_file))\n",
    "    print(os.path.join(glob.UC_DATA_DIR, target_file))\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19,)\n",
      "(9,)\n",
      "/home/alexv84/Documents/Arbeit/Allianz/AZVers/data/train.spacy\n",
      "/home/alexv84/Documents/Arbeit/Allianz/AZVers/data/valid.spacy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<spacy.tokens._serialize.DocBin at 0x7fa58c6d3430>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[\"text_cleaned\"].values, X[\"target\"].values, test_size=0.3)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "cats = [\"0\",\"1\"]\n",
    "\n",
    "make_docs(list(zip(X_train, y_train)), \"train.spacy\", cats=cats)\n",
    "make_docs(list(zip(X_test, y_test)), \"valid.spacy\", cats=cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
      "/home/alexv84/Documents/Arbeit/Allianz/AZVers/data/config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy init fill-config /home/alexv84/Documents/Arbeit/Allianz/AZVers/data/base_config_textclf.cfg /home/alexv84/Documents/Arbeit/Allianz/AZVers/data/config.cfg   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Saving to output directory:\n",
      "/home/alexv84/Documents/Arbeit/Allianz/AZVers/data/output\u001b[0m\n",
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'textcat']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
      "E    #       LOSS TOK2VEC  LOSS TEXTCAT  CATS_SCORE  SCORE \n",
      "---  ------  ------------  ------------  ----------  ------\n",
      "  0       0          0.00          0.25       64.94    0.65\n",
      " 10     200          7.57         22.78       64.94    0.65\n",
      " 21     400         62.92          6.69       58.46    0.58\n",
      " 31     600         14.36          4.55       75.00    0.75\n",
      " 42     800          0.00          0.00       75.00    0.75\n",
      " 52    1000          0.00          0.00       75.00    0.75\n",
      " 63    1200          0.00          0.00       75.00    0.75\n",
      " 73    1400          0.00          0.00       75.00    0.75\n",
      " 84    1600          0.00          0.00       75.00    0.75\n",
      " 94    1800          0.00          0.00       75.00    0.75\n"
     ]
    }
   ],
   "source": [
    "from spacy.cli.train import train\n",
    "\n",
    "train(config_path = f\"{glob.UC_DATA_DIR}/config.cfg\",\n",
    "        output_path = f\"{glob.UC_DATA_DIR}/output\",\n",
    "        overrides={\"paths.train\": f\"{glob.UC_DATA_DIR}/train.spacy\",\n",
    "                \"paths.dev\": f\"{glob.UC_DATA_DIR}/valid.spacy\",\n",
    "    },\n",
    ")\n",
    "\n",
    "print(\"Training done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use best model fit:\n",
    "trained_nlp = spacy.load(f\"{glob.UC_DATA_DIR}/output/model-best\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 2\n",
    "\n",
    "text = X['text_cleaned'][id]\n",
    "print(text)\n",
    "print(X['target'][id])\n",
    "\n",
    "# Perform the trained pipeline on this text\n",
    "doc = trained_nlp(text)\n",
    "\n",
    "doc.cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_from_pretrained = nlp(text)\n",
    "\n",
    "print(\"entities\", doc_from_pretrained.ents)\n",
    "print(\"sentences\", list(doc_from_pretrained.sents))\n",
    "print(\"classification\", doc_from_pretrained.cats)    # empty -> add trained_nlp pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"de_core_news_lg\", exclude=[\"textcat\"])\n",
    "nlp.add_pipe(\"textcat\", source = trained_nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Companies, agencies, institutions, etc.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"ORG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Countries, cities, states'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"GPE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#displacy.render(text1,style=\"ent\",jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL=\"https://www.zeebiz.com/markets/currency/news-cryptocurrency-news-today-june-12-bitcoin-dogecoin-shiba-inu-and-other-top-coins-prices-and-all-latest-updates-158490\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_content = requests.get(URL).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html_content, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "body=soup.body.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "body= body.replace('\\n', ' ')\n",
    "#body= body.replace('t', ' ')\n",
    "#body= body.replace('r', ' ')\n",
    "#body= body.replace('xa0', ' ')\n",
    "#body=re.sub(r'[^ws]', '', body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "text3= NER(body)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('env_pdf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d49ecafdcaaa7827583b5b21ff598afa0c2165a9797f363685e472c1c2a4bb3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
