{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract text from Pdf documents and apply NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber, os\n",
    "import os.path as osp\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from pdf_extract.services import file\n",
    "from pdf_extract.config import global_config as glob\n",
    "from pdf_extract.config import config\n",
    "from pdf_extract.utils import utils\n",
    "from importlib import reload\n",
    "import fasttext\n",
    "\n",
    "reload(glob)\n",
    "reload(config)\n",
    "reload(utils);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "list_of_NEG_docs = os.listdir(osp.join(glob.UC_DATA_DIR, 'example_cvs', 'negatives'))\n",
    "n_neg = len(list_of_NEG_docs)\n",
    "\n",
    "list_of_POS_docs = os.listdir(osp.join(glob.UC_DATA_DIR, 'example_cvs', 'positives'))\n",
    "n_pos = len(list_of_POS_docs)\n",
    "\n",
    "print(n_pos)\n",
    "print(n_neg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fname = 'CV4'\n",
    "\n",
    "# i, page_objects, text = 0, {}, []\n",
    "# # The open method returns an instance of the pdfplumber.PDF class.\n",
    "# with pdfplumber.open(osp.join(glob.UC_DATA_DIR, f\"example_cvs/{fname}.pdf\")) as pdf:\n",
    "#     while i < len(pdf.pages):\n",
    "#         page = pdf.pages[i]\n",
    "#         page_objects[str(i+1)] = page.extract_text(x_tolerance=1).split('\\n')\n",
    "#         #mystring = page_objects[str(i+1)].replace('\\r', '').replace('\\n', '')\n",
    "#         text += page_objects[str(i+1)]\n",
    "#         print(f\"Page {i}\")\n",
    "#         #print(page.extract_text())\n",
    "#         i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#page_objects['1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in all Pdfs in directory:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Postives only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_corpus1 = pd.DataFrame(columns=['text', 'fname'])\n",
    "\n",
    "for z, fname in enumerate(list_of_POS_docs):\n",
    "    i, page_objects, text = 0, {}, \"\"\n",
    "    # The open method returns an instance of the pdfplumber.PDF class.\n",
    "    with pdfplumber.open(osp.join(glob.UC_DATA_DIR, \"example_cvs\",f\"positives/{fname}\")) as pdf:\n",
    "        while i < len(pdf.pages):\n",
    "            page = pdf.pages[i]\n",
    "            #print(pdf.metadata)\n",
    "            page_objects[str(i+1)] = page.extract_text(x_tolerance=1, y_tolerance=3) #.split('\\n')\n",
    "            text += page_objects[str(i+1)]\n",
    "            #print(f\"Page {i}\")\n",
    "            #print(page.extract_text())\n",
    "            i += 1\n",
    "    \n",
    "    raw_corpus1.loc[z] = [text,fname]  \n",
    "raw_corpus1['label'] = 'positive'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negatives only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_corpus2 = pd.DataFrame(columns=['text', 'fname'])\n",
    "\n",
    "for z, fname in enumerate(list_of_NEG_docs):\n",
    "    i, page_objects, text = 0, {}, \"\"\n",
    "    # The open method returns an instance of the pdfplumber.PDF class.\n",
    "    with pdfplumber.open(osp.join(glob.UC_DATA_DIR, \"example_cvs\",f\"negatives/{fname}\")) as pdf:\n",
    "        while i < len(pdf.pages):\n",
    "            page = pdf.pages[i]\n",
    "            page_objects[str(i+1)] = page.extract_text(x_tolerance=1, y_tolerance=3) #.split('\\n')\n",
    "            text += page_objects[str(i+1)]\n",
    "            i += 1\n",
    "    \n",
    "    raw_corpus2.loc[z] = [text,fname]   \n",
    "raw_corpus2['label'] = 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_corpus = pd.concat([raw_corpus1,raw_corpus2], axis=0, ignore_index=True)\n",
    "#raw_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#page_objects\n",
    "\n",
    "#page.extract_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#page_instance = page_objects['1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pdf.metadata\n",
    "#page_instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess text corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using english language.\n",
      "Using 179 stop words.\n",
      "Added 8 stopword(s).\n",
      "Added 13 stopword(s).\n",
      "Adding custom stop words...\n",
      "Setting to lower cases.\n",
      "Removing whitespaces.\n",
      "Applying word tokenizer.\n",
      "Removing custom stopwords.\n",
      "Removing punctuations.\n",
      "Removing numbers.\n",
      "Removing digits.\n",
      "Removing non-alphabetic characters.\n",
      "Removing short tokens.\n",
      "Finished preprocessing!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/alexv84/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/alexv84/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from pdf_extract.resources import preprocessor as preproc\n",
    "\n",
    "reload(preproc)\n",
    "\n",
    "# Preprocess corpus:\n",
    "cleaner = preproc.clean_text(language='english', lemma = False, stem = False)\n",
    "\n",
    "X_raw = raw_corpus['text'].copy()\n",
    "\n",
    "# Full sample\n",
    "X_cleaned = cleaner.fit_transform(X_raw)   \n",
    "\n",
    "#X = raw_corpus['text']\n",
    "\n",
    "#X_cl = cleaner.fit_transform(X_raw)\n",
    "\n",
    "#docs = X_cl.tolist()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare supervised FastText data set(s):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_fct = lambda x: '__label__{}'.format(x.label)\n",
    "\n",
    "raw_corpus['text_cleaned'] = X_cleaned\n",
    "raw_corpus['fasttext_label'] = raw_corpus.apply(combine_fct, axis = 1)\n",
    "\n",
    "X = raw_corpus[['fasttext_label', 'text_cleaned']].reset_index(drop=True)   # keep label in X for txt-file below, will not be used directly in ft\n",
    "#X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 2)\n",
      "(6, 2)\n",
      "(6, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "reload(utils)\n",
    "\n",
    "X_train, X_valid, X_test, y_train, y_valid, y_test = utils.train_test_split_extend(X, X['fasttext_label'], test_size=[0.2, 0.2], random_state=42, shuffle=True)\n",
    "#X_train, X_test, y_train, y_test = utils.train_test_split_extend(X, X['fasttext_label'], test_size=[0.2], random_state=42, shuffle=True)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(X_valid.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare preprocessed data for fasttext training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TXT Service output to file: /home/alexv84/Documents/Arbeit/Allianz/AZVers/data/train_data_fasttext.txt\n",
      "TXT Service output to file: /home/alexv84/Documents/Arbeit/Allianz/AZVers/data/test_data_fasttext.txt\n",
      "TXT Service output to file: /home/alexv84/Documents/Arbeit/Allianz/AZVers/data/dev_data_fasttext.txt\n"
     ]
    }
   ],
   "source": [
    "reload(file)\n",
    "\n",
    "config_output = config.io['output']\n",
    "Path(glob.UC_DATA_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Train:\n",
    "file.TXTService(verbose=True, root_path=glob.UC_DATA_DIR, **config_output['service']['TXTService1']).doWrite(X_train)\n",
    "\n",
    "# Test:\n",
    "file.TXTService(verbose=True, root_path=glob.UC_DATA_DIR, **config_output['service']['TXTService2']).doWrite(X_test)\n",
    "\n",
    "# Dev:\n",
    "file.TXTService(verbose=True, root_path=glob.UC_DATA_DIR, **config_output['service']['TXTService3']).doWrite(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : dim is manually set to a specific value. It will not be automatically optimized.\n",
      "Progress: 100.0% Trials:  859 Best score:  1.000000 ETA:   0h 0m 0s\n",
      "Training again with best arguments\n",
      "Read 0M words\n",
      "Number of words:  2295\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread:    8614 lr:  0.000000 avg.loss:  0.695756 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "#-------------\n",
    "\n",
    "# Training word embeddings: \n",
    "#model = fasttext.train_supervised(input = os.path.join(glob.UC_DATA_DIR,'train_data_fasttext.txt'), wordNgrams = 2, dim=100, epoch=30, lr=1.0, verbose=2)\n",
    "\n",
    "# Autotune hyperparameter:\n",
    "model = fasttext.train_supervised(input=os.path.join(glob.UC_DATA_DIR,'train_data_fasttext.txt'), autotuneValidationFile=os.path.join(glob.UC_DATA_DIR,'dev_data_fasttext.txt'), dim=100, autotuneMetric=\"f1:__label__positive\", autotuneDuration=300, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 0.8333333333333334, 0.8333333333333334)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.test(os.path.join(glob.UC_DATA_DIR,'dev_data_fasttext.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__negative',), array([0.50001019]))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(\"I am a cook with 10 years of cooking experience. My hobbies are sports and cycling.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__positive',), array([0.50001019]))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(\"I am a statistician from the university of Erlangen-Nuremberg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get predictions for train/test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(row):\n",
    "    yhat, score = model.predict(row['text_cleaned'])\n",
    "    return yhat[0]\n",
    "\n",
    "y_train_pred = X_train.apply(predict,axis=1)\n",
    "y_test_pred = X_test.apply(predict,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "__label__negative       1.00      0.89      0.94         9\n",
      "__label__positive       0.88      1.00      0.93         7\n",
      "\n",
      "         accuracy                           0.94        16\n",
      "        macro avg       0.94      0.94      0.94        16\n",
      "     weighted avg       0.95      0.94      0.94        16\n",
      "\n",
      "Test:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "__label__negative       0.33      0.33      0.33         3\n",
      "__label__positive       0.33      0.33      0.33         3\n",
      "\n",
      "         accuracy                           0.33         6\n",
      "        macro avg       0.33      0.33      0.33         6\n",
      "     weighted avg       0.33      0.33      0.33         6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print('Train:')\n",
    "print(classification_report(y_train, y_train_pred, target_names=['__label__negative', '__label__positive']))\n",
    "print('Test:')\n",
    "print(classification_report(y_test, y_test_pred, target_names=['__label__negative', '__label__positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#page_instance.pdf.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#page_instance.extract_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individually explain predictions on test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "class_names = ['negative', 'positive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import webbrowser\n",
    "from pathlib import Path\n",
    "\n",
    "def fasttext_prediction_in_sklearn_format(classifier, texts):\n",
    "    res = []\n",
    "    # Ask FastText for the top 10 most likely labels for each piece of text.\n",
    "    # This ensures we always get a probability score for every possible label in our model.\n",
    "    labels, probabilities = classifier.predict(texts, 10)\n",
    "\n",
    "    # For each prediction, sort the probabaility scores into the same order\n",
    "    # This is needed because FastText\n",
    "    # returns predicitons sorted by most likely instead of in a fixed order.\n",
    "    for label, probs, text in zip(labels, probabilities, texts):\n",
    "        order = np.argsort(np.array(label))\n",
    "        res.append(probs[order])\n",
    "\n",
    "    return np.array(res)\n",
    "\n",
    "\n",
    "# LIME needs to be able to mimic how the classifier splits\n",
    "# the string into words. So we'll provide a function that\n",
    "# mimics how FastText works.\n",
    "def tokenize_string(string):\n",
    "    return string.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a LimeTextExplainer. This object knows how to explain a text-based\n",
    "# prediction by dropping words randomly.\n",
    "explainer = LimeTextExplainer(\n",
    "    # We need to tell LIME how to split the string into words. We can do this\n",
    "    # by giving it a function to call to split a string up the same way FastText does it.\n",
    "    split_expression=tokenize_string,\n",
    "    # Our FastText classifer uses bigrams (two-word pairs) to classify text. Setting\n",
    "    # bow=False tells LIME to not assume that our classifier is based on single words only.\n",
    "    bow=False,\n",
    "    # To make the output pretty, tell LIME what to call each possible prediction from our model.\n",
    "    class_names=class_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: __label__negative\n",
      "Probability(\"positive\") = 0.5000100135803223\n",
      "True class: __label__positive\n"
     ]
    }
   ],
   "source": [
    "idx = 2\n",
    "\n",
    "# Make a prediction and explain it!\n",
    "exp = explainer.explain_instance(\n",
    "    X_test['text_cleaned'].iloc[idx],\n",
    "    classifier_fn=lambda x: fasttext_prediction_in_sklearn_format(model, x),\n",
    "    num_features=20\n",
    ")\n",
    "\n",
    "#print(f\"Document name: {X_test['fasttext_label'].iloc[idx]}\")\n",
    "print('Predicted: %s' % model.predict(X_test['text_cleaned'].iloc[idx])[0][0])\n",
    "print('Probability(\"positive\") =', model.predict(X_test['text_cleaned'].iloc[idx])[1][0])\n",
    "print('True class: %s' % y_test.iloc[idx])\n",
    "\n",
    "\n",
    "# Plot local token importances:\n",
    "#-------------------------------\n",
    "%matplotlib inline\n",
    "#fig = exp.as_pyplot_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted features for linear surrogate model, which approximates the behaviour of the orig. classifier in the vicinity of the test example.\n",
    "#exp.as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "NER = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use pretrained language identification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'__label__de'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fasttext\n",
    "\n",
    "model = fasttext.load_model(osp.join(glob.UC_LANG_ID, 'lid.176.bin'))\n",
    "\n",
    "label, score = model.predict(\"Hallo ich studiere Mathe in München\", k=2)  # top 2 matching languages\n",
    "label[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "#raw_text = X.loc[i]\n",
    "raw_text = X_cl.loc[i]\n",
    "#raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text=\"The Indian Space Research Organisation or is the national space agency of India, headquartered in Bengaluru. It operates under Department of Space which is directly overseen by the Prime Minister of India while Chairman of ISRO acts as executive of DOS as well.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = NER(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for word in text1.ents:\n",
    "#     print(word.text,word.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Companies, agencies, institutions, etc.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"ORG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Countries, cities, states'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"GPE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#displacy.render(text1,style=\"ent\",jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL=\"https://www.zeebiz.com/markets/currency/news-cryptocurrency-news-today-june-12-bitcoin-dogecoin-shiba-inu-and-other-top-coins-prices-and-all-latest-updates-158490\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_content = requests.get(URL).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html_content, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "body=soup.body.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "body= body.replace('\\n', ' ')\n",
    "#body= body.replace('t', ' ')\n",
    "#body= body.replace('r', ' ')\n",
    "#body= body.replace('xa0', ' ')\n",
    "#body=re.sub(r'[^ws]', '', body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "text3= NER(body)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('env_pdf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d49ecafdcaaa7827583b5b21ff598afa0c2165a9797f363685e472c1c2a4bb3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
