{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract text from Pdf documents and apply NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Variable: UC_CODE_DIR has been set to default: /home/alexv84/Documents/GitHub/pdf_extract/src\n",
      "Environment Variable: UC_DATA_DIR has been set to default: /home/alexv84/Documents/Arbeit/Allianz/AZVers/data\n",
      "Environment Variable: UC_LANG_ID has been set to default: /home/alexv84/Documents/Arbeit/Allianz/AZVers/fasttext_langdetect\n",
      "Environment Variable: UC_DATA_PKG_DIR has been set to default: \n",
      "Environment Variable: UC_DB_CONNECTION has been set to default: postgresql://postgres...\n",
      "Environment Variable: UC_PORT has been set to default: 5000\n",
      "Environment Variable: UC_APP_CONNECTION has been set to default: 127.0.0.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'pdf_extract.utils.utils' from '/home/alexv84/Documents/GitHub/pdf_extract/src/pdf_extract/utils/utils.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pdfplumber, os\n",
    "import os.path as osp\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from pdf_extract.services import file\n",
    "from pdf_extract.config import global_config as glob\n",
    "from pdf_extract.config import config\n",
    "from pdf_extract.utils import utils\n",
    "from importlib import reload\n",
    "import fasttext\n",
    "\n",
    "reload(glob)\n",
    "reload(config)\n",
    "reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "list_of_NEG_docs = os.listdir(osp.join(glob.UC_DATA_DIR, 'example_cvs', 'negatives'))\n",
    "n_neg = len(list_of_NEG_docs)\n",
    "\n",
    "list_of_POS_docs = os.listdir(osp.join(glob.UC_DATA_DIR, 'example_cvs', 'positives'))\n",
    "n_pos = len(list_of_POS_docs)\n",
    "\n",
    "print(n_pos)\n",
    "print(n_neg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fname = 'CV4'\n",
    "\n",
    "# i, page_objects, text = 0, {}, []\n",
    "# # The open method returns an instance of the pdfplumber.PDF class.\n",
    "# with pdfplumber.open(osp.join(glob.UC_DATA_DIR, f\"example_cvs/{fname}.pdf\")) as pdf:\n",
    "#     while i < len(pdf.pages):\n",
    "#         page = pdf.pages[i]\n",
    "#         page_objects[str(i+1)] = page.extract_text(x_tolerance=1).split('\\n')\n",
    "#         #mystring = page_objects[str(i+1)].replace('\\r', '').replace('\\n', '')\n",
    "#         text += page_objects[str(i+1)]\n",
    "#         print(f\"Page {i}\")\n",
    "#         #print(page.extract_text())\n",
    "#         i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#page_objects['1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in all Pdfs in directory:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Postives only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_corpus1 = pd.DataFrame(columns=['text', 'label'])\n",
    "\n",
    "for z, fname in enumerate(list_of_POS_docs):\n",
    "    #print(fname)\n",
    "    i, page_objects, text = 0, {}, \"\"\n",
    "    # The open method returns an instance of the pdfplumber.PDF class.\n",
    "    with pdfplumber.open(osp.join(glob.UC_DATA_DIR, \"example_cvs\",f\"positives/{fname}\")) as pdf:\n",
    "        while i < len(pdf.pages):\n",
    "            page = pdf.pages[i]\n",
    "            page_objects[str(i+1)] = page.extract_text(x_tolerance=1, y_tolerance=3) #.split('\\n')\n",
    "            text += page_objects[str(i+1)]\n",
    "            #print(f\"Page {i}\")\n",
    "            #print(page.extract_text())\n",
    "            i += 1\n",
    "    \n",
    "    raw_corpus1.loc[z] = text  \n",
    "raw_corpus1['label'] = 'positive'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negatives only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_corpus2 = pd.DataFrame(columns=['text','label'])\n",
    "\n",
    "for z, fname in enumerate(list_of_NEG_docs):\n",
    "    #print(fname)\n",
    "    i, page_objects, text = 0, {}, \"\"\n",
    "    # The open method returns an instance of the pdfplumber.PDF class.\n",
    "    with pdfplumber.open(osp.join(glob.UC_DATA_DIR, \"example_cvs\",f\"negatives/{fname}\")) as pdf:\n",
    "        while i < len(pdf.pages):\n",
    "            page = pdf.pages[i]\n",
    "            page_objects[str(i+1)] = page.extract_text(x_tolerance=1, y_tolerance=3) #.split('\\n')\n",
    "            text += page_objects[str(i+1)]\n",
    "            #print(f\"Page {i}\")\n",
    "            #print(page.extract_text())\n",
    "            i += 1\n",
    "    \n",
    "    raw_corpus2.loc[z] = text   \n",
    "raw_corpus2['label'] = 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_corpus = pd.concat([raw_corpus1,raw_corpus2], axis=0, ignore_index=True)\n",
    "#raw_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#page_objects\n",
    "\n",
    "#page.extract_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#page_instance = page_objects['1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pdf.metadata\n",
    "#page_instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess text corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using english language.\n",
      "Using 179 stop words.\n",
      "Added 0 stopword(s).\n",
      "Adding custom stop words...\n",
      "Setting to lower cases.\n",
      "Removing whitespaces.\n",
      "Applying word tokenizer.\n",
      "Removing custom stopwords.\n",
      "Removing punctuations.\n",
      "Removing numbers.\n",
      "Removing digits.\n",
      "Removing non-alphabetic characters.\n",
      "Removing short tokens.\n",
      "Finished preprocessing!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/alexv84/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/alexv84/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/alexv84/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/alexv84/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from pdf_extract.resources import preprocessor as preproc\n",
    "\n",
    "reload(preproc)\n",
    "\n",
    "# Preprocess corpus:\n",
    "cleaner = preproc.clean_text(language='english', lemma = False, stem = False)\n",
    "\n",
    "X = raw_corpus['text']\n",
    "\n",
    "X_cl = cleaner.fit_transform(X)\n",
    "\n",
    "#docs = X_cl.tolist()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare supervised FastText data set(s):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_fct = lambda x: '__label__{}'.format(x.label)\n",
    "\n",
    "raw_corpus['text_cleaned'] = X_cl\n",
    "raw_corpus['fasttext_label'] = raw_corpus.apply(combine_fct, axis = 1)\n",
    "\n",
    "X = raw_corpus[['fasttext_label', 'text_cleaned']].reset_index(drop=True)   # keep label in X for txt-file below, will not be used directly in ft\n",
    "#X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 2)\n",
      "(5, 2)\n",
      "(5, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "reload(utils)\n",
    "\n",
    "X_train, X_valid, X_test, y_train, y_valid, y_test = utils.train_test_split_extend(X, X['fasttext_label'], test_size=[0.2, 0.2], random_state=42, shuffle=True)\n",
    "#X_train, X_test, y_train, y_test = utils.train_test_split_extend(X, X['fasttext_label'], test_size=[0.2], random_state=42, shuffle=True)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(X_valid.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare preprocessed data for fasttext training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TXT Service output to file: /home/alexv84/Documents/Arbeit/Allianz/AZVers/data/train_data_fasttext.txt\n",
      "TXT Service output to file: /home/alexv84/Documents/Arbeit/Allianz/AZVers/data/test_data_fasttext.txt\n",
      "TXT Service output to file: /home/alexv84/Documents/Arbeit/Allianz/AZVers/data/dev_data_fasttext.txt\n"
     ]
    }
   ],
   "source": [
    "reload(file)\n",
    "\n",
    "config_output = config.io['output']\n",
    "Path(glob.UC_DATA_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Train:\n",
    "file.TXTService(verbose=True, root_path=glob.UC_DATA_DIR, **config_output['service']['TXTService1']).doWrite(X_train)\n",
    "\n",
    "# Test:\n",
    "file.TXTService(verbose=True, root_path=glob.UC_DATA_DIR, **config_output['service']['TXTService2']).doWrite(X_test)\n",
    "\n",
    "# Dev:\n",
    "file.TXTService(verbose=True, root_path=glob.UC_DATA_DIR, **config_output['service']['TXTService3']).doWrite(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : dim is manually set to a specific value. It will not be automatically optimized.\n",
      "Progress: 100.0% Trials:  343 Best score:  1.000000 ETA:   0h 0m 0s\n",
      "Training again with best arguments\n",
      "Read 0M words\n",
      "Number of words:  2225\n",
      "Number of labels: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100.0% words/sec/thread:    8130 lr:  0.000000 avg.loss:  0.694452 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "#-------------\n",
    "\n",
    "# Training word embeddings: \n",
    "#model = fasttext.train_supervised(input = os.path.join(glob.UC_DATA_DIR,'train_data_fasttext.txt'), wordNgrams = 2, dim=100, epoch=30, lr=1.0, verbose=2)\n",
    "\n",
    "# Autotune hyperparameter:\n",
    "model = fasttext.train_supervised(input=os.path.join(glob.UC_DATA_DIR,'train_data_fasttext.txt'), autotuneValidationFile=os.path.join(glob.UC_DATA_DIR,'dev_data_fasttext.txt'), dim=100, autotuneMetric=\"f1:__label__positive\", autotuneDuration=300, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1.0, 1.0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.test(os.path.join(glob.UC_DATA_DIR,'dev_data_fasttext.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__negative',), array([0.50001007]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(\"I am a cook with 10 years of cooking experience. My hobbies are sports and cycling.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__negative',), array([0.50001013]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(\"I am a statistician from the university of Erlangen-Nuremberg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get predictions for train/test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(row):\n",
    "    yhat, score = model.predict(row['text_cleaned'])\n",
    "    return yhat[0]\n",
    "\n",
    "y_train_pred = X_train.apply(predict,axis=1)\n",
    "y_test_pred = X_test.apply(predict,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "__label__negative       0.80      1.00      0.89         8\n",
      "__label__positive       1.00      0.71      0.83         7\n",
      "\n",
      "         accuracy                           0.87        15\n",
      "        macro avg       0.90      0.86      0.86        15\n",
      "     weighted avg       0.89      0.87      0.86        15\n",
      "\n",
      "Test:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "__label__negative       1.00      0.67      0.80         3\n",
      "__label__positive       0.67      1.00      0.80         2\n",
      "\n",
      "         accuracy                           0.80         5\n",
      "        macro avg       0.83      0.83      0.80         5\n",
      "     weighted avg       0.87      0.80      0.80         5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print('Train:')\n",
    "print(classification_report(y_train, y_train_pred, target_names=['__label__negative', '__label__positive']))\n",
    "print('Test:')\n",
    "print(classification_report(y_test, y_test_pred, target_names=['__label__negative', '__label__positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#page_instance.pdf.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#page_instance.extract_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "NER = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use pretrained language identification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'__label__de'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fasttext\n",
    "\n",
    "model = fasttext.load_model(osp.join(glob.UC_LANG_ID, 'lid.176.bin'))\n",
    "\n",
    "label, score = model.predict(\"Hallo ich studiere Mathe in München\", k=2)  # top 2 matching languages\n",
    "label[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "#raw_text = X.loc[i]\n",
    "raw_text = X_cl.loc[i]\n",
    "#raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text=\"The Indian Space Research Organisation or is the national space agency of India, headquartered in Bengaluru. It operates under Department of Space which is directly overseen by the Prime Minister of India while Chairman of ISRO acts as executive of DOS as well.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = NER(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for word in text1.ents:\n",
    "#     print(word.text,word.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Companies, agencies, institutions, etc.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"ORG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Countries, cities, states'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"GPE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#displacy.render(text1,style=\"ent\",jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL=\"https://www.zeebiz.com/markets/currency/news-cryptocurrency-news-today-june-12-bitcoin-dogecoin-shiba-inu-and-other-top-coins-prices-and-all-latest-updates-158490\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_content = requests.get(URL).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html_content, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "body=soup.body.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "body= body.replace('\\n', ' ')\n",
    "#body= body.replace('t', ' ')\n",
    "#body= body.replace('r', ' ')\n",
    "#body= body.replace('xa0', ' ')\n",
    "#body=re.sub(r'[^ws]', '', body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "text3= NER(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('env_pdf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d49ecafdcaaa7827583b5b21ff598afa0c2165a9797f363685e472c1c2a4bb3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
